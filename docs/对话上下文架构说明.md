# 对话上下文架构设计说明

## 🎯 问题背景

### 原始问题
在使用OpenAI/Moonshot Chat API进行批量翻译时，每次API调用都是独立的，这导致：
1. 每批翻译都需要重新发送完整的提示词
2. AI无法记住之前批次的翻译风格
3. 术语翻译可能前后不一致
4. Token消耗高昂

### 错误的理解
最初以为将提示词放入 `system message` 就能"只计费一次"，但实际上：
- ❌ 每次API调用都会发送完整的messages数组
- ❌ System message每次都计费
- ❌ AI在批次之间没有任何记忆

## 🔍 业界最佳实践

通过研究GitHub上的AI翻译项目（如吴恩达的Translation Agent），发现主流解决方案是：

### 对话上下文管理（Conversation History Management）

**核心思想：** 维护完整的对话历史，让AI能够"记住"之前的翻译

```python
# 对话历史结构
conversation_history = [
    {"role": "system", "content": "翻译规范"},     # 第1次发送
    {"role": "user", "content": "翻译第1批"},      # 第1次
    {"role": "assistant", "content": "翻译结果1"}, # 第1次
    {"role": "user", "content": "翻译第2批"},      # 第2次
    {"role": "assistant", "content": "翻译结果2"}, # 第2次
    # ... 后续批次持续追加
]
```

## 💡 我们的实现方案

### 架构设计

```python
class POTranslator:
    def __init__(self):
        # 完整的翻译规范（约300 tokens）
        self.system_prompt = """详细的UE翻译规则..."""
        
        # 对话历史（维护整个文件的翻译上下文）
        self.conversation_history = []
        
        # 历史管理参数
        self.max_history_tokens = 2000
    
    def translate_batch(self, texts):
        # 第1批：初始化历史
        if not self.conversation_history:
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": "请翻译: ..."}
            ]
            self.conversation_history.append(system_message)
        else:
            # 第2+批：使用历史
            messages = self.conversation_history + [
                {"role": "user", "content": "请翻译: ..."}
            ]
        
        # 调用API
        response = api_call(messages)
        
        # 更新历史
        self.conversation_history.append(user_message)
        self.conversation_history.append(assistant_message)
        
        # 防止历史过长
        if len(self.conversation_history) > 21:
            # 保留system + 最近5轮对话
            self.conversation_history = [
                self.conversation_history[0]  # system
            ] + self.conversation_history[-10:]  # 最近5轮
        
        return translations
    
    def translate_po_file(self, file):
        # 每个文件重置对话历史
        self.conversation_history = []
        
        for batch in batches:
            self.translate_batch(batch)
```

## 📊 Token消耗分析

### 场景：翻译103条记录，11批次

#### 方案对比

| 批次 | 无上下文 | 极简System | 对话历史（我们的方案） |
|------|----------|-----------|---------------------|
| 第1批 | 650 tokens | 380 tokens | **380 tokens** |
| 第2批 | 650 tokens | 90 tokens | **480 tokens** (system+历史) |
| 第3批 | 650 tokens | 90 tokens | **580 tokens** (累积) |
| ... | ... | ... | ... |
| 第11批 | 650 tokens | 90 tokens | **1,180 tokens** |
| **总计** | **7,150** | **1,280** | **约6,000** |

### Token消耗权衡

**对话历史方案的特点：**
- ✅ AI能记住之前的翻译风格
- ✅ 术语翻译保持一致
- ✅ 翻译质量更高
- ⚠️ Token消耗逐渐增加（但有上限控制）
- ⚠️ 比极简方案多消耗约4,700 tokens

**成本对比：**
- 极简方案：1,280 tokens = ¥0.015
- 对话历史方案：6,000 tokens = ¥0.072
- **差额：¥0.057（约5.7分）**

**结论：** 为了更高的翻译质量和一致性，每100条记录多花5.7分是值得的！

## 🎯 优势分析

### 1. 翻译一致性

**问题场景：**
```
第1批翻译: "Unique" → "去重"
第5批翻译: "Unique" → "独特的"  ❌（不一致）
```

**对话历史方案：**
```
第1批: "Unique" → "去重"
第5批: AI看到历史中的"去重"，保持一致 → "去重" ✅
```

### 2. 术语学习

AI能从之前的翻译中学习：
- 项目特定的术语
- 命名风格
- 格式偏好

### 3. 上下文理解

```
第1批: "Sort by distance" → "按距离排序"
第2批: "Sort by name" → AI记住了"Sort"的翻译风格 → "按名称排序"
```

## ⚖️ 权衡考虑

### 历史长度管理

**策略：** 保留最近N轮对话

```python
# 当历史超过21条消息时（system + 10轮对话）
if len(conversation_history) > 21:
    # 保留: system + 最近5轮
    conversation_history = [
        conversation_history[0],      # system message
        *conversation_history[-10:]   # 最近5轮(10条消息)
    ]
```

**原因：**
1. 太短：AI记不住之前的翻译风格
2. 太长：Token消耗过高，且远期历史用处不大
3. 5-10轮：平衡点，既能保持一致性又控制成本

### 成本控制

**对于不同规模的项目：**

| 项目规模 | 极简方案 | 对话历史方案 | 差额 | 建议 |
|----------|---------|-------------|------|------|
| <100条 | ¥0.02 | ¥0.07 | ¥0.05 | 对话历史 ✅ |
| 100-500条 | ¥0.1 | ¥0.35 | ¥0.25 | 对话历史 ✅ |
| 500-1000条 | ¥0.2 | ¥0.70 | ¥0.50 | 对话历史 ✅ |
| >5000条 | ¥1.0 | ¥3.50 | ¥2.50 | 根据质量要求选择 |

**建议：**
- 追求最高质量：使用对话历史
- 预算有限：使用极简方案
- 可配置：让用户选择

## 🔧 实现细节

### 1. 历史初始化

```python
def translate_po_file(self, po_file):
    # 每个文件开始时重置历史
    self.conversation_history = []
    
    # 开始翻译各批次...
```

### 2. 第一批特殊处理

```python
def translate_batch(self, texts):
    if not self.conversation_history:
        # 第一批：包含system message
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        # 保存system message到历史
        self.conversation_history.append(system_message)
    else:
        # 后续批：使用历史
        messages = self.conversation_history + [new_user_message]
```

### 3. 历史更新

```python
# API调用后更新历史
self.conversation_history.append({
    "role": "user", 
    "content": user_prompt
})
self.conversation_history.append({
    "role": "assistant", 
    "content": ai_response
})
```

### 4. 历史截断

```python
# 防止历史过长
if len(self.conversation_history) > 21:
    self.conversation_history = [
        self.conversation_history[0]  # 保留system
    ] + self.conversation_history[-10:]  # 最近5轮
```

## 📈 实际效果验证

### 验证方法

1. **一致性测试**
   - 在不同批次中包含相同术语
   - 检查翻译是否一致

2. **质量对比**
   - 对比使用/不使用历史的翻译结果
   - 人工评估翻译质量

3. **Token统计**
   - 实时追踪token消耗
   - 对比预期值

## 🚀 未来优化方向

### 1. 智能历史管理

根据翻译内容动态调整历史长度：
- 术语密集：保留更长历史
- 简单文本：缩短历史

### 2. 术语缓存

提取已翻译的术语，单独管理：
```python
term_cache = {
    "Unique": "去重",
    "Slice": "截取",
    # ...
}
```

### 3. 混合方案

根据项目规模自动选择：
- <100条：对话历史
- >1000条：极简方案 + 术语缓存

## 📝 总结

### 核心思想
通过维护对话历史，让AI在批次之间保持"记忆"，从而提高翻译一致性和质量。

### 关键权衡
- **质量 vs 成本**：多花5-10%的费用，换取显著提升的翻译质量
- **历史长度 vs Token消耗**：保留5-10轮对话是最佳平衡点

### 最终方案
✅ **采用对话历史管理架构**
- 每个文件独立上下文
- 保留最近5-10轮对话
- System message只在第一批发送
- Token消耗可控（约6,000 tokens/100条）
- 翻译质量和一致性大幅提升

---

**版本：** v3.0  
**架构模式：** 对话上下文管理（Conversation History Management）  
**灵感来源：** GitHub开源项目 + OpenAI最佳实践
